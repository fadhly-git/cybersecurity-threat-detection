{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69dc7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from src.data.loader import DataLoader\n",
    "from src.data.preprocessing import DataPreprocessor\n",
    "from src.utils.helpers import load_config\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67096755",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa7de68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = load_config('../config/config.yaml')\n",
    "\n",
    "# Load dataset\n",
    "loader = DataLoader()\n",
    "df = loader.load_dataset('../data/raw/cybersecurity_attacks.csv')\n",
    "\n",
    "print(f\"Original dataset shape: {df.shape}\")\n",
    "target_col = 'attack_type' if 'attack_type' in df.columns else df.columns[-1]\n",
    "print(f\"Target column: {target_col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7ce868",
   "metadata": {},
   "source": [
    "## Stage 1: Remove Redundant Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daa4aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = DataPreprocessor()\n",
    "\n",
    "# Remove redundant columns\n",
    "df_stage1 = preprocessor.remove_redundant_columns(\n",
    "    df.copy(), \n",
    "    correlation_threshold=0.95, \n",
    "    variance_threshold=0.01\n",
    ")\n",
    "\n",
    "removed_cols = set(df.columns) - set(df_stage1.columns)\n",
    "print(f\"Removed {len(removed_cols)} columns\")\n",
    "print(f\"Remaining: {df_stage1.shape[1]} columns\")\n",
    "\n",
    "if removed_cols:\n",
    "    print(f\"\\nRemoved columns: {list(removed_cols)[:10]}...\" if len(removed_cols) > 10 else f\"\\nRemoved columns: {list(removed_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd4e046",
   "metadata": {},
   "source": [
    "## Stage 2: Encode Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3041ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical columns\n",
    "categorical_cols = df_stage1.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "if target_col in categorical_cols:\n",
    "    categorical_cols.remove(target_col)\n",
    "\n",
    "print(f\"Categorical columns: {categorical_cols[:5]}...\" if len(categorical_cols) > 5 else f\"Categorical columns: {categorical_cols}\")\n",
    "\n",
    "# Encode\n",
    "df_stage2, encoders = preprocessor.encode_categorical(\n",
    "    df_stage1.copy(),\n",
    "    categorical_columns=categorical_cols,\n",
    "    method='onehot'\n",
    ")\n",
    "\n",
    "print(f\"\\nShape after encoding: {df_stage2.shape}\")\n",
    "print(f\"Added {df_stage2.shape[1] - df_stage1.shape[1]} encoded features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c96219",
   "metadata": {},
   "source": [
    "## Stage 3: Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449b0f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values before\n",
    "missing_before = df_stage2.isnull().sum().sum()\n",
    "print(f\"Missing values before: {missing_before}\")\n",
    "\n",
    "# Handle missing values\n",
    "df_stage3 = preprocessor.handle_missing_values(\n",
    "    df_stage2.copy(),\n",
    "    strategy='mean'\n",
    ")\n",
    "\n",
    "missing_after = df_stage3.isnull().sum().sum()\n",
    "print(f\"Missing values after: {missing_after}\")\n",
    "print(f\"‚úÖ Imputed {missing_before - missing_after} missing values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25535ff9",
   "metadata": {},
   "source": [
    "## Stage 4 & 5: Detect and Handle Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9068028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect outliers\n",
    "outlier_mask = preprocessor.detect_outliers(\n",
    "    df_stage3.copy(),\n",
    "    method='isolation_forest',\n",
    "    contamination=0.1\n",
    ")\n",
    "\n",
    "n_outliers = outlier_mask.sum()\n",
    "print(f\"Detected {n_outliers} outlier samples ({n_outliers/len(df_stage3)*100:.2f}%)\")\n",
    "\n",
    "# Visualize outliers\n",
    "if n_outliers > 0:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(['Normal', 'Outlier'], [len(df_stage3) - n_outliers, n_outliers], \n",
    "            color=['green', 'red'], alpha=0.7)\n",
    "    plt.title('Outlier Detection Results')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaee52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle outliers\n",
    "df_stage5 = preprocessor.handle_outliers(\n",
    "    df_stage3.copy(),\n",
    "    outlier_mask=outlier_mask,\n",
    "    method='cap'  # Cap outliers instead of removing\n",
    ")\n",
    "\n",
    "print(f\"Shape after handling outliers: {df_stage5.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8063a98c",
   "metadata": {},
   "source": [
    "## Stage 6: Standardize Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b705d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df_stage5.drop(columns=[target_col])\n",
    "y = df_stage5[target_col]\n",
    "\n",
    "# Show before standardization\n",
    "print(\"Before standardization:\")\n",
    "print(X.iloc[:, :3].describe())\n",
    "\n",
    "# Standardize\n",
    "X_scaled, scaler = preprocessor.standardize_features(\n",
    "    X.copy(),\n",
    "    method='standard'\n",
    ")\n",
    "\n",
    "print(\"\\nAfter standardization:\")\n",
    "print(pd.DataFrame(X_scaled, columns=X.columns).iloc[:, :3].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd3df9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize standardization effect\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Before\n",
    "axes[0].boxplot([X.iloc[:, i] for i in range(min(5, X.shape[1]))], \n",
    "                labels=X.columns[:5])\n",
    "axes[0].set_title('Before Standardization')\n",
    "axes[0].set_ylabel('Value')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# After\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "axes[1].boxplot([X_scaled_df.iloc[:, i] for i in range(min(5, X.shape[1]))], \n",
    "                labels=X.columns[:5])\n",
    "axes[1].set_title('After Standardization')\n",
    "axes[1].set_ylabel('Value')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b1d761",
   "metadata": {},
   "source": [
    "## Stage 7: Handle Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408e5c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class distribution before\n",
    "print(\"Class distribution before balancing:\")\n",
    "print(y.value_counts())\n",
    "print(f\"\\nImbalance ratio: {y.value_counts().max() / y.value_counts().min():.2f}:1\")\n",
    "\n",
    "# Apply SMOTE\n",
    "X_balanced, y_balanced = preprocessor.handle_class_imbalance(\n",
    "    X_scaled,\n",
    "    y,\n",
    "    method='smote'\n",
    ")\n",
    "\n",
    "print(\"\\nClass distribution after balancing:\")\n",
    "print(pd.Series(y_balanced).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236497ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class balance\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=('Before SMOTE', 'After SMOTE'),\n",
    "    specs=[[{'type': 'bar'}, {'type': 'bar'}]]\n",
    ")\n",
    "\n",
    "# Before\n",
    "counts_before = y.value_counts()\n",
    "fig.add_trace(\n",
    "    go.Bar(x=counts_before.index.astype(str), y=counts_before.values,\n",
    "           marker_color='lightcoral', name='Before'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# After\n",
    "counts_after = pd.Series(y_balanced).value_counts()\n",
    "fig.add_trace(\n",
    "    go.Bar(x=counts_after.index.astype(str), y=counts_after.values,\n",
    "           marker_color='lightgreen', name='After'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(title_text='Class Balance Comparison', showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db42e5b",
   "metadata": {},
   "source": [
    "## Stage 8: Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bca8879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = preprocessor.split_data(\n",
    "    X_balanced,\n",
    "    y_balanced,\n",
    "    test_size=0.2,\n",
    "    stratify=True\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"\\nTraining class distribution:\")\n",
    "print(pd.Series(y_train).value_counts())\n",
    "print(f\"\\nTest class distribution:\")\n",
    "print(pd.Series(y_test).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ccad83",
   "metadata": {},
   "source": [
    "## üéØ Complete Pipeline Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6f044c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run entire pipeline in one go\n",
    "print(\"Running complete 7-stage pipeline...\\n\")\n",
    "\n",
    "preprocessor_full = DataPreprocessor()\n",
    "X_train_full, X_test_full, y_train_full, y_test_full = preprocessor_full.run_pipeline(\n",
    "    df.copy(),\n",
    "    target_column=target_col,\n",
    "    correlation_threshold=0.95,\n",
    "    variance_threshold=0.01,\n",
    "    encoding_method='onehot',\n",
    "    missing_strategy='mean',\n",
    "    outlier_detection_method='isolation_forest',\n",
    "    outlier_handling_method='cap',\n",
    "    scaling_method='standard',\n",
    "    balance_method='smote',\n",
    "    test_size=0.2\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Pipeline complete!\")\n",
    "print(f\"Training set: {X_train_full.shape}\")\n",
    "print(f\"Test set: {X_test_full.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd0829e",
   "metadata": {},
   "source": [
    "## üìä Preprocessing Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4651ffea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary\n",
    "summary = {\n",
    "    'Stage': [\n",
    "        '1. Remove Redundant',\n",
    "        '2. Encode Categorical',\n",
    "        '3. Handle Missing',\n",
    "        '4-5. Handle Outliers',\n",
    "        '6. Standardize',\n",
    "        '7. Balance Classes',\n",
    "        '8. Split Data'\n",
    "    ],\n",
    "    'Action': [\n",
    "        f'Removed {len(removed_cols)} columns',\n",
    "        f'Encoded {len(categorical_cols)} features',\n",
    "        f'Imputed {missing_before} values',\n",
    "        f'Handled {n_outliers} outliers',\n",
    "        f'Standardized {X.shape[1]} features',\n",
    "        f'SMOTE: {len(y)} ‚Üí {len(y_balanced)} samples',\n",
    "        f'80/20 split: {len(y_train)}/{len(y_test)}'\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary)\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502dd75e",
   "metadata": {},
   "source": [
    "## üíæ Save Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d63e64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessed data\n",
    "import os\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "\n",
    "np.save('../data/processed/X_train.npy', X_train_full)\n",
    "np.save('../data/processed/X_test.npy', X_test_full)\n",
    "np.save('../data/processed/y_train.npy', y_train_full)\n",
    "np.save('../data/processed/y_test.npy', y_test_full)\n",
    "\n",
    "print(\"‚úÖ Preprocessed data saved to data/processed/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6c0a4a",
   "metadata": {},
   "source": [
    "## üìù Next Steps\n",
    "\n",
    "1. ‚úÖ Data is now ready for model training\n",
    "2. Proceed to **03_model_training.ipynb** to train ML and DL models\n",
    "3. Experiment with different preprocessing parameters\n",
    "4. Try different balancing methods (ADASYN, undersampling)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
